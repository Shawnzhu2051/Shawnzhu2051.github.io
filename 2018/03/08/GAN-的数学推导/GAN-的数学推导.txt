---
title: GAN 的数学推导
date: 2018-03-08 23:09:41
tags: "机器学习"
mathjax: true
---

最近将研究方向转移到了很火热很有趣的 生成式对抗网络 (Generative Adversarial Nets, GAN)上面来了。GAN是Goodfellow于2014年提出的一项基于博弈论和minimax理论的对抗性生成模型。这篇文章在老板的督促下，翻来覆去起码看了无数遍，所有的公式也手工推导了两到三遍，算得上是小有一些体会。

关于GAN的原理上的阐述，在网上的各种帖子博客里已经说得很详细了。这篇文章主要针对于GAN中比较关键的几个数学公式，予以证明。

<!-- more -->

### Introduction

首先给出GAN的目标函数：

$\min_{G}\max_{D} V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log{D(x)}] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1-D(G(z)))]$

其中，$p_{data}(x)$ 是真实数据的数据分布，$p_{z}(z)$是先验的噪音数据分布，$p_{g}(x)$是生成数据的数据分布，$D(\cdot)$表示判别器认为这个输入数据为真实数据的概率。这个目标函数想要达到的目标是使生成数据的数据分布$p_{g}(x)$和真实数据的数据分布$p_{data}(x)$相同，即生成器可以生成以假乱真的图像。为此，我们需要最优化生成器$G$和判别器$D$，即：

$D^{\*}_{G} = \text{arg}\max_{D}V(G,D)$和$G^{\*} = \text{arg}\min_{G}V(G,D^{\*}_{G})$

更新算法如下：

![GANalg](GAN-的数学推导/GANalg.png)

### Q1：如何衡量$p_{g}(x)$ 和$p_{data}(x)$的相近程度？

为了论证$p_{g}(x) = p_{data}(x)$，我们需要设置一个衡量两个数据分布相似程度的指标。在文中，Goodfellow引入了KL散度：

$D_{KL}(P||Q) = \mathbb{E}_{x \sim p}[\log{\frac{P(x)}{Q(x)}}] = \mathbb{E}_{x \sim p}[\log{P(x)}-\log{Q(x)}]$.

为什么KL散度可以衡量$p_{g}(x)$ 和$p_{data}(x)$的相近程度呢？下面来证明。

我们从$P_{data}(x)$中抽取$m$个样本${x^{1},}x^{2},...,x^{m}$,设分布的参数为$\theta$，则每个样本可以表示为$p_{g}(x;\theta)$，得到似然函数：

$L = \prod^{m}_{i=1}p_{g}(x^{i};\theta)$。

该似然函数的含义是：在由$\theta$确定的生成分布中，$m$个样本全部出现的概率。

接下来我们将最大化似然函数来得到最优分布$\theta^{\*}$：

$\theta^{\*} = \text{arg}\max_{\theta}\prod^{m}_{i=1}p_{g}(x^{i};\theta)$

因为$\log(\cdot)$函数是单调的，所以为去掉连乘，将似然函数化为

$\theta^{\*} = \text{arg}\max_{\theta}\log{\prod^{m}_{i=1}p_{g}(x^{i};\theta)}  \\\\= \text{arg}\max_{\theta}\sum^{m}_{i=1}log{p_{g}(x^{i};\theta)} \\\\ = \text{arg}\min_{\theta}\mathbb{E}_{x \sim p_{data}}[\log{p_{g}(x;\theta)}]$

（因为$x^{i} \in p_{data}(x)$）

添加一项不含$\theta$的项，并展开：

$\theta^{\*} = \text{arg}\max_{\theta}\int_{x}p_{data}(x)\log{p_{g}(x;\theta)}dx - \int_{x}p_{data}(x)\log{p_{data}(x)}dx \\\\= \text{arg}\max_{\theta}\int_{x}p_{data}(x)\log{\frac{p_{g}(x;\theta)}{p_{data}(x)}} dx \\\\  = \text{arg}\max_{\theta} KL(p_{g}(x;\theta)|| p_{data}(x))$

由此证明了KL散度能够衡量$p_{g}(x)$ 和$p_{data}(x)$的相近程度。



### Q2：如何证明判别器$D$存在最优解？最优解的值为多少？

展开GAN的目标函数中数学期望部分可以得到：

$V(G,D) = \int_{x}p_{data}(x)\log{D(x)}dx + \int_{z}p_{z}(z)\log{(1-D(G(z))}dz$.

当给定一个$G$时，因为$G$输入的是噪音数据$z$，得到的是生成的数据$x$，所以使用换元积分，令$G(z) = x$。值得注意的是，该处证明存在一个漏洞，即要使用换元积分的话，函数$G(x)$必须是可逆的，然而$G$是一个神经网络，目前尚不能证明其实可逆的。但该种换元方法在机器学习中非常常见，所以就暂时忽略掉。

$\int_{x}p_{data}(x)\log{D(x)}dx + \int_{x}p_{g}(x)log{(1-D(x))}dxV(G,D) \\\\= \int_{x}\left[p_{data}(x)\log{D(x)} + p_{g}(x)log{(1-D(x))}\right]dx $

因为要求目标函数$V$的最大值，而$G$是固定的，所以最大值只和$D$有关。为方便起见，令$p_{data}(x)  = a, p_{g}(x) = b, D(x) = y, V(\cdot) = f(\cdot)$ .

则$f(y) = a \log{y} + b \log{(1-y)}$

$f^{'}(y) = \frac{a}{y} - \frac{b}{1-y} = 0$ 

$y = \frac{a}{a+b}$

二阶导数：$f^{''}(y) = -\frac{a}{y^{2}}-\frac{b}{1-y^{2}}$

 $f^{''}(\frac{a}{a+b}) \lt 0 ,(a,b \in (0,1))$.

所以证明了$y = \frac{a}{a+b}$为极大值且唯一，即$D^{\*}_{G}(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{g}(x)}$

如果最后GAN能够达到最优，即$p_{g}(x) = p_{data}(x)$，则判别器$D^{\*}_{G}(x) = \frac{1}{2}$！

也就是说判别器最后对输入数据，不论是真实数据还是生成数据，判定其为真的概率均为$\frac{1}{2}$，也就是说随机猜测无法判断。那么我们可以认为这个生成数据已经能够以假乱真，让判别器无法判断了。



### Q3：如何证明当且仅当$p_{g}(x) = p_{data}(x)$时，生成器$G$有最优解？

这个当且仅当问题需要从两个方向来证明。

我们先假设当判别器$D$训练好时，固定$D$，令$C(G) = V(D^{\*}_{G},G)$，再令$p_{g}(x) = p_{data}(x)$来求$C(G)$的一个可能的取值，再证明它是最优解：

$C(G) = \int_{x}p_{data}(x)\log{\frac{1}{2}} + p_{g}(x)\log{\frac{1}{2}} dx \\\\=-\log{2}\int_{x}p_{data}(x)dx - \log{2}\int_{x}p_{g}(x)dx \\ =-log{4}.$

接下来对所有的$G$，将最优判别器$D^{\*}_{G}$代入到$C(G)$中，来求$C(G)$的最优解：

$C(G) = \int_{x} p_{data}(x)\log{\frac{p_{data}(x)}{p_{g}(x)+p_{data}(x)}} + p_{g}(x)\log{\frac{p_{g}(x)}{p_{g}(x)+p_{data}(x)}}dx \\\\= \int_{x}(\log{2} - \log{2})p_{data}(x) + p_{data}(x)\log{\frac{p_{data}(x)}{p_{g}(x)+p_{data}(x)}} + (\log{2} - \log{2})p_{g}(x) + p_{g}(x)\log{\frac{p_{g}(x)}{p_{g}(x)+p_{data}(x)}}dx$ 

我们在这里使用了一个小技巧，添加了两项值为0的项，并展开，对原式的结果没有影响。

$C(G) = -\log{2}\int_{x}[p_{g}(x) + p_{data}(x)]dx + \int_{x}\left[p_{data}(x)\left(\log{2} + \log{\frac{p_{data}(x)}{p_{g}(x)+p_{data}(x)}}\right) + p_{g}(x)\left(\log{2}+\log{\frac{p_{g}(x)}{p_{g}(x)+p_{data}(x)}} \right)\right]dx \\\\= -\log{4} + \int_{x}\left[p_{data}(x)\left(\log{\frac{p_{data}(x)}{\frac{p_{g}(x)+p_{data}(x)}{2}}}\right) + p_{g}(x)\left(\log{\frac{p_{g}(x)}{\frac{p_{g}(x)+p_{data}(x)}{2}}} \right)\right]dx \\\\= -\log{4} + KL(p_{data}||\frac{p_{data}+p_{g}}{2}) + KL(p_{g}||\frac{p_{data}+p_{g}}{2}).$

因为KL散度是恒大于零的，所以$-\log{4}$为$C(G)$的最小值。

接下来再证明当生成器$G$取得最优解$-\log{4}$时，$p_{g}(x) = p_{data}(x)$。

因为KL散度不可以交换，所以文中引入了JS散度。JS散度的定义如下。

$JSD(P||Q) = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M)$，其中$M = \frac{1}{2}(P+Q)$，$JSD \in [0, \log{2}]$.

所以$C(G) = -\log{4} + 2\cdot JSD(p_{data}||p_{g})$.

当$p_{data}(x) = p_{g}(x)$时，$JSD(p_{data}||p_{g}) = 0$。

所以当生成器$G$取得最优解$-\log{4}$时，$p_{g}(x) = p_{data}(x)$。



### Summary

* GAN结合了生成模型和判别模型，消除了生成模型的loss function难以定义的问题。
* 基于概率分布来计算，不受生成维度的限制。
* 可以用来进行半监督学习（先使用无标签的数据训练GAN，在基于GAN使用有标签的数据训练判别器）。
* 论文有严格的数学证明。



